{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80f19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference\n",
    "# [1] Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. \"Semantics derived automatically from language corpora contain human-like biases.\" Science 356, no. 6334 (2017): 183-186.\n",
    "# [2] https://psychbruce.github.io/PsychWordVec/reference/test_WEAT.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29e168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b702ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading word vector\n",
    "def load_glove_vector(filename):\n",
    "    word2vec = {}\n",
    "    num_lines = sum(1 for _ in open(filename, 'r', encoding='utf-8'))\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=num_lines, desc=\"Loading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                word2vec[word] = vector\n",
    "            except ValueError as e:\n",
    "                1\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffae804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 100%|██████████| 2196017/2196017 [01:35<00:00, 23099.15it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = load_glove_vector('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e02465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generic Functions ---\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c25214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WEAT-related Functions ---\n",
    "\n",
    "# Compute the association of word w with A & B\n",
    "# refer: s_w(w, A, B) measures the association of w with the attribute\n",
    "def s_w(w, A, B, embeddings):\n",
    "    # Compute the cosine similarity between word w and every word in A\n",
    "    similarities_with_A = [cosine_similarity(embeddings[w], embeddings[a]) for a in A]\n",
    "    mean_sim_with_A = np.mean(similarities_with_A)\n",
    "    \n",
    "    # Compute the cosine similarity between word w and every word in B\n",
    "    similarities_with_B = [cosine_similarity(embeddings[w], embeddings[b]) for b in B]\n",
    "    mean_sim_with_B = np.mean(similarities_with_B)\n",
    "    \n",
    "    return mean_sim_with_A - mean_sim_with_B\n",
    "\n",
    "# Compute the differential association between two sets of words (X & Y) and A & B\n",
    "# refer: s_group(X,Y,A,B) measures the differential association of the two sets of target words with the attribute\n",
    "def s_group(X, Y, A, B, embeddings):\n",
    "    associations_with_X = [s_w(x, A, B, embeddings) for x in X]\n",
    "    total_association_X = np.sum(associations_with_X)\n",
    "    \n",
    "    associations_with_Y = [s_w(y, A, B, embeddings) for y in Y]\n",
    "    total_association_Y = np.sum(associations_with_Y)\n",
    "    \n",
    "    return total_association_X - total_association_Y\n",
    "\n",
    "# Calculate effect size\n",
    "def effect_size(X, Y, A, B, embeddings):\n",
    "    associations_with_X = [s_w(x, A, B, embeddings) for x in X]\n",
    "    mean_association_X = np.mean(associations_with_X)\n",
    "    \n",
    "    associations_with_Y = [s_w(y, A, B, embeddings) for y in Y]\n",
    "    mean_association_Y = np.mean(associations_with_Y)\n",
    "    \n",
    "    all_associations = associations_with_X + associations_with_Y\n",
    "    standard_deviation = np.std(all_associations, ddof=1)  # Using sample standard deviation\n",
    "    \n",
    "    return (mean_association_X - mean_association_Y) / standard_deviation\n",
    "\n",
    "# Calculate one-side p-value\n",
    "def p_value(X, Y, A, B, embeddings, n_iterations):\n",
    "    combined = X + Y\n",
    "    count_exceeding_original = 0  # Count the number of times exceeding the original s_group value\n",
    "    original_s = s_group(X, Y, A, B, embeddings) # Calculate the original s_group value\n",
    "\n",
    "    for _ in tqdm(range(n_iterations), desc=\"Calculating P value\"):\n",
    "        np.random.shuffle(combined)\n",
    "        \n",
    "        # The array after permutation becomes the new X and Y\n",
    "        shuffled_X = combined[:len(X)]\n",
    "        shuffled_Y = combined[len(X):]\n",
    "        \n",
    "        # If the s_group value after permutation is greater than the original, increment the counter\n",
    "        if s_group(shuffled_X, shuffled_Y, A, B, embeddings) > original_s:\n",
    "            count_exceeding_original += 1\n",
    "\n",
    "    return count_exceeding_original / n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae2c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT Experiment\n",
    "\n",
    "target1= ['woman', 'mother'] # X\n",
    "target2= ['man', 'father'] # Y\n",
    "attribute1= ['health', 'happy'] # A\n",
    "attribute2= ['pollute', 'tragedy'] # B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c269ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating P value:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating P value: 100%|██████████| 1000/1000 [00:00<00:00, 4344.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size: 1.1705615520477295\n",
      "P Value: 0.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# WEAT\n",
    "# Compute the WEAT effect-size and p-value both with the normal distribution generated by 1,000 iterations\n",
    "\n",
    "es = effect_size(target1, target2, attribute1, attribute2, glove_vectors)\n",
    "p_val = p_value(target1, target2, attribute1, attribute2, glove_vectors, n_iterations=1000)\n",
    "print(f\"Effect Size: {es}\")\n",
    "print(f\"P Value: {p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2156ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating P value: 100%|██████████| 1000000/1000000 [03:39<00:00, 4565.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size: 1.1705615520477295\n",
      "P Value: 0.166081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# WEAT\n",
    "# The empirical distribution generated by 1,000,000 iterations\n",
    "\n",
    "es = effect_size(target1, target2, attribute1, attribute2, glove_vectors)\n",
    "p_val = p_value(target1, target2, attribute1, attribute2, glove_vectors, n_iterations=1000000)\n",
    "print(f\"Effect Size: {es}\")\n",
    "print(f\"P Value: {p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9eaeb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating P value: 100%|██████████| 1000/1000 [00:12<00:00, 77.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size: 0.9262961745262146\n",
      "P Value: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# WEAT\n",
    "# Here I changed some words to test for obvious bias, to see if it's significant.\n",
    "# Here the p-value is indeed < 0.05\n",
    "\n",
    "target1= [\"woman\", \"female\", \"girl\", \"lady\", \"daughter\", \"sister\", \"she\", \"her\", \"wife\", \"mother\", \"aunt\", \"niece\", \"grandmother\", \"bride\", \"madam\", \"mrs\", \"miss\", \"queen\", \"princess\", \"damsel\"]#X\n",
    "target2= [\"man\", \"male\", \"boy\", \"gentleman\", \"son\", \"brother\", \"he\", \"him\", \"husband\", \"father\", \"uncle\", \"nephew\", \"grandfather\", \"groom\", \"sir\", \"mr\", \"king\", \"prince\", \"lord\", \"duke\"] #Y\n",
    "attribute1= [\"home\", \"family\", \"children\", \"parent\", \"kitchen\", \"domestic\", \"nurture\", \"caring\", \"house\", \"marriage\", \"homemaker\", \"household\", \"birth\", \"rearing\", \"nanny\", \"caretaker\", \"cook\", \"clean\", \"maid\", \"housewife\"] #A\n",
    "attribute2= [\"work\", \"career\", \"office\", \"professional\", \"salary\", \"job\", \"business\", \"corporate\", \"manager\", \"executive\", \"profession\", \"ceo\", \"entrepreneur\", \"employee\", \"occupation\", \"labor\", \"workplace\", \"boss\", \"hire\", \"industry\"] #B\n",
    "\n",
    "es = effect_size(target1, target2, attribute1, attribute2, glove_vectors)\n",
    "p_val = p_value(target1, target2, attribute1, attribute2, glove_vectors, n_iterations=1000)\n",
    "print(f\"Effect Size: {es}\")\n",
    "print(f\"P Value: {p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44f452d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SC-WEAT-related Functions ---\n",
    "\n",
    "# Compute the association of word w with attributes A & B\n",
    "def s_w_sc_weat(w, A, B, embeddings):\n",
    "\n",
    "    similarities_with_A = [cosine_similarity(embeddings[w], embeddings[a]) for a in A]\n",
    "    mean_sim_with_A = np.mean(similarities_with_A)\n",
    "    \n",
    "    similarities_with_B = [cosine_similarity(embeddings[w], embeddings[b]) for b in B]\n",
    "    mean_sim_with_B = np.mean(similarities_with_B)\n",
    "    \n",
    "    all_similarities = similarities_with_A + similarities_with_B\n",
    "    standard_deviation = np.std(all_similarities, ddof=1)  # Using sample standard deviation\n",
    "    \n",
    "    return (mean_sim_with_A - mean_sim_with_B) / standard_deviation\n",
    "\n",
    "# Compute the average effect size for all target words\n",
    "def effect_size_sc_weat(target, A, B, embeddings):\n",
    "    associations_with_target = [s_w_sc_weat(w, A, B, embeddings) for w in target]\n",
    "    mean_association = np.mean(associations_with_target)\n",
    "    return mean_association\n",
    "\n",
    "# Analogous to WEAT, calculate p-value by permuting attributes A and B.\n",
    "# This is a bit different from the approach in the Reference paper. In the reference paper, each word has a property (p_w), and linear regression is used for prediction.\n",
    "# Here, while we are given a target_list of words, there is no pw provided. Hence, we still take an approach analogous to WEAT, permuting A and B, to determine significance.\n",
    "\n",
    "def p_value_sc_weat(target, A, B, embeddings, n_iterations):\n",
    "    combined_attributes = A + B\n",
    "    original_effect_size = effect_size_sc_weat(target, A, B, embeddings)\n",
    "    count_exceeding_original = 0  # Count exceeding the original effect size\n",
    "    \n",
    "    for _ in tqdm(range(n_iterations), desc=\"Computing p-value\"):\n",
    "        np.random.shuffle(combined_attributes)\n",
    "        \n",
    "        # The attribute list after permutation becomes the new A and B\n",
    "        shuffled_A = combined_attributes[:len(A)]\n",
    "        shuffled_B = combined_attributes[len(A):]\n",
    "        \n",
    "        # If the effect size after permutation is greater than the original effect size, increment the counter\n",
    "        if effect_size_sc_weat(target, shuffled_A, shuffled_B, embeddings) > original_effect_size:\n",
    "            count_exceeding_original += 1\n",
    "\n",
    "    return count_exceeding_original / n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57554c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SC-WEAT Experiment\n",
    "# \"AylinCaliskan\" isn't a single word; couldn't find it in glove_vectors. So here, I've split it into two words: first name and last name.\n",
    "\n",
    "target = [\"Xinyu\", \"weat\", \"Aylin\", \"Caliskan\"] \n",
    "attribute1 = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\", \"happy\", \"laughter\", \"paradise\", \"vacation\"]\n",
    "attribute2 = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\", \"vomit\", \"agony\", \"prison\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c6d6a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing p-value:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing p-value: 100%|██████████| 1000/1000 [00:01<00:00, 592.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size: -0.20858444273471832\n",
      "P Value: 0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the SC-WEAT (aka WEFAT) effect-size and p-value with an empirical distribution generated via 1,000,000 permutations for the case sensitive words\n",
    "# 1000 permutations as a quick test\n",
    "\n",
    "es_sc_weat = effect_size_sc_weat(target, attribute1, attribute2, glove_vectors)\n",
    "p_val_sc_weat = p_value_sc_weat(target, attribute1, attribute2, glove_vectors, n_iterations=1000)\n",
    "print(f\"Effect Size: {es_sc_weat}\")\n",
    "print(f\"P Value: {p_val_sc_weat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36952d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing p-value: 100%|██████████| 1000000/1000000 [27:56<00:00, 596.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size: -0.20858444273471832\n",
      "P Value: 0.815381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1,000,000 permutations\n",
    "\n",
    "es_sc_weat = effect_size_sc_weat(target, attribute1, attribute2, glove_vectors)\n",
    "p_val_sc_weat = p_value_sc_weat(target, attribute1, attribute2, glove_vectors, n_iterations=1000000)\n",
    "print(f\"Effect Size: {es_sc_weat}\")\n",
    "print(f\"P Value: {p_val_sc_weat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95337286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing p-value: 100%|██████████| 1000/1000 [00:06<00:00, 145.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect Size: 1.0810129642486572\n",
      "P Value: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SC-WEAT\n",
    "# Here I changed some words to test for obvious bias, to see if it's significant.\n",
    "# Indeed, the p-value here is < 0.05.\n",
    "\n",
    "target= [\"woman\", \"female\", \"girl\", \"lady\", \"daughter\", \"sister\", \"she\", \"her\", \"wife\", \"mother\", \"aunt\", \"niece\", \"grandmother\", \"bride\", \"madam\", \"mrs\", \"miss\", \"queen\", \"princess\", \"damsel\"]#X\n",
    "attribute1= [\"home\", \"family\", \"children\", \"parent\", \"kitchen\", \"domestic\", \"nurture\", \"caring\", \"house\", \"marriage\", \"homemaker\", \"household\", \"birth\", \"rearing\", \"nanny\", \"caretaker\", \"cook\", \"clean\", \"maid\", \"housewife\"] #A\n",
    "attribute2= [\"work\", \"career\", \"office\", \"professional\", \"salary\", \"job\", \"business\", \"corporate\", \"manager\", \"executive\", \"profession\", \"ceo\", \"entrepreneur\", \"employee\", \"occupation\", \"labor\", \"workplace\", \"boss\", \"hire\", \"industry\"] #B\n",
    "\n",
    "es_sc_weat = effect_size_sc_weat(target, attribute1, attribute2, glove_vectors)\n",
    "p_val_sc_weat = p_value_sc_weat(target, attribute1, attribute2, glove_vectors, n_iterations=1000)\n",
    "print(f\"Effect Size: {es_sc_weat}\")\n",
    "print(f\"P Value: {p_val_sc_weat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
